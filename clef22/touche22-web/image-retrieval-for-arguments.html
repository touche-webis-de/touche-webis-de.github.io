---
layout: default
nav_active: shared-tasks
title: Touché at CLEF 2022 - Image Retrieval for Arguments
description: Touché at CLEF 2022 - Image Retrieval for Arguments
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="../../index.html">Touché</a></li>
<li><a href="../../shared-tasks.html">Shared Tasks</a></li>
<li class="uk-disabled"><a href="#">Image Retrieval for Arguments 2022</a></li>
</ul>
</nav>

<main>
<div class="uk-section uk-section-default">
<div class="uk-container uk-margin-small">
<h1 class="uk-margin-remove-top uk-margin-remove-bottom">Image Retrieval for Arguments 2022</h1>
<ul class="uk-list">
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#data">Data</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#evaluation">Evaluation</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#submission">Submission</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#tira-quickstart">TIRA Quickstart</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#results">Results</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#related-work">Related Work</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
</ul>
</div>

<div class="uk-container uk-margin-medium">

<h2 id="synopsis">Synopsis</h2>
<ul>
<li>Task: Given a controversial topic, the task is to retrieve images (from web pages) for each stance (pro/con) that show support for that stance. [<a href="https://www.tira.io/c/touche/9">forum</a>]</li>
<li>Input: [<a href="https://doi.org/10.5281/zenodo.6786948">data</a>]</li>
<li>Evaluation: [<a href="https://zenodo.org/record/6786949/files/topics.xml?download=1">topics</a>] [<a href="https://images.args.me">browser</a>]</li>
<li>Submission: [<a href="https://github.com/touche-webis-de/touche-code/blob/main/clef22/image-retrieval-for-arguments/original-rankings-baseline.py">baseline</a>] [verifier: <a href="https://github.com/touche-webis-de/touche-code/blob/main/clef22/image-retrieval-for-arguments/submission-verifier.sh">program</a>, <a href="https://zenodo.org/record/6786949/files/image-ids.txt?download=1">image-ids</a>] [<a href="https://www.tira.io/task/touche-task-3/dataset/touche-task-3-2022-01-21">tira</a>]</li>
<li>Manual judgments: [<a href="https://zenodo.org/record/6786949/files/touche-task3-001-050-relevance.qrels?download=1">qrels</a>] [<a href="https://github.com/touche-webis-de/touche-code/blob/main/clef22/image-retrieval-for-arguments/submission-evaluator.sh">evaluator</a>]</li>
</ul>

<h2 id="data">Data</h2>
<p>This task uses a focused crawl of about 20,000 images (and associated web pages) as document collection. See the collection's <a href="https://zenodo.org/record/6786949/files/README.txt">README</a> for more information on its contents and file formats. [<a href="https://doi.org/10.5281/zenodo.6786948">download</a>]</p>

<h2 id="evaluation">Evaluation</h2>
<p>Systems are evaluated on Touché topics 1–50 by the ratio of images among the 20 retrieved images for each topic (10 images for each stance) that are all three: relevant to the topic, argumentative, and have the associated stance. The file format is explained in the <a href="https://zenodo.org/record/6786949/files/README.txt">README</a>). [<a href="https://zenodo.org/record/6786949/files/topics.xml?download=1">topics</a>]</p>

<h2 id="submission">Submission</h2>
<p>We encourage participants to use <a href="https://www.tira.io/">TIRA</a> for their submissions to allow for a better reproducibility (see the Quickstart section below). Email submission is allowed as a fallback. For each topic and stance, include 10 retrieved images. Each team can submit up to 5 different runs.</p>
<p>The submission format adapts the standard TREC format. Each line corresponds to an image retrieved for some topic and stance at a certain rank, making a run file 1000 lines long (50 topics, 2 stances, 10 ranks). Each line contains the following fields, separated by single whitespaces: [verifier: <a href="https://github.com/touche-webis-de/touche-code/blob/main/clef22/image-retrieval-for-arguments/submission-verifier.sh">program</a>, <a href="https://zenodo.org/record/6786949/files/image-ids.txt?download=1">image-ids</a>]</p>
<ul>
<li>The topic number (1 to 50).</li>
<li>The stance ("PRO" or "CON").</li>
<li>The image's ID (corresponds to the name of the image's directory in the collection; always 17 characters long and starts with "I").</li>
<li>The rank (1 to 10 in increasing order per topic and stance). Not used in this year's evaluation.</li>
<li>A score (integer or floating point; non-increasing per topic and stance). Not used in this year's evaluation.</li>
<li>A tag that identifies your group and the method you used to produce the run.</li>
</ul>
For example:
<pre>
1 PRO I000330ba4ea0ad13 1 17.89 myGroupMyMethod
1 PRO I0005e6fe00ea17fd 2 16.43 myGroupMyMethod
...
1 CON I0009d5f038fe6f2e 1 15.89 myGroupMyMethod
1 CON I000f34bd3f8cb030 2 14.43 myGroupMyMethod
...
</pre>
<p>If you have questions, please ask in the <a href="https://www.tira.io/c/touche/9">forum</a>. You will get a combined TIRA-and-forum account on registration.</p>
<p>We provide relevance judgements for submitted runs as binary judgements on topic-image pairs for topic-relevance, the pro-stance, and the con-stance. [<a href="https://zenodo.org/record/6786949/files/touche-task3-001-050-relevance.qrels?download=1">qrels</a>] [<a href="https://github.com/touche-webis-de/touche-code/blob/main/clef22/image-retrieval-for-arguments/submission-evaluator.sh">evaluator</a>]</p>

<h2 id="tira-quickstart">TIRA Quickstart</h2>
<p>Participant software is run in a virtual machine. Log in to <a href="https://www.tira.io">TIRA</a>, go to the <a href="https://www.tira.io/task/touche-task-1/dataset/touche-task-1-2022-02-16">task's dataset page</a>, and click on "&gt;_ SUBMIT". Click the "CONNECTION INFO" button for how to connect to the virtual machine. Click on "POWER ON" if the state is not "RUNNING".</p>
<img width="75%" src="../touche22-figures/task2-tira-vm-overview.png" alt="Virtual machine state in TIRA.">
<p>The software is executed on the command line with two parameters: (1) <code>$inputDataset</code> refers to a directory that contains the <a href="#data">collection</a>; (2) <code>$outputDir</code> refers to a directory in which the software has to create the <a href="#submission">submission file</a> named <code>run.txt</code>. Specify exactly how each software of your virtual machine is run using the "Command" field in the TIRA web interface:
<img width="75%" src="../touche22-figures/task2-tira-example-software.png" alt="Software configuration in TIRA.">
<p>As you "RUN" the software, you will not be able to connect to the virtual machine (takes at least 10 minutes). Once finished, click on "INSPECT" to check on the run and click on "EVALUATE" for a syntax check (give it a few minutes, then check back on the page). Your run will later be reviewed and evaluated by the organizers. If uncertain on something, ask in the <a href="https://www.tira.io/c/touche/9">forum</a> or send a mail/message to <a href="#task-committee">Johannes</a>.</p>
<img width="75%" src="../touche22-figures/task2-tira-example-evaluation.png" alt="A run in TIRA.">
<p>Create a separate "Software" entry in the TIRA web interface for each of your approaches. NOTE: By submitting your software you retain full copyrights. You agree to grant us usage rights for evaluation of the corresponding data generated by your software. We agree not to share your software with a third party or use it for any purpose other than research.</p>


<h2 id="results">Results</h2>
<table class="uk-table uk-table-divider uk-table-small uk-table-hover">
  <caption>Best-scoring run of each team (same for each score) [<a href="https://zenodo.org/record/6802946/files/results-per-topic.zip?download=1">per-topic</a>] [<a href="https://zenodo.org/record/6786949/files/touche-task3-001-050-relevance.qrels?download=1">qrels</a>] [<a href="https://images.args.me">browser</a>]</caption>
<thead>
<tr><th>Team</th><th>Tag</th><th colspan="3">Precision@10</th></tr>
<tr><th></th><th></th><th>On topic</th><th>Argumentative</th><th>On stance</th></tr>
</thead>
<tbody>
<tr><td><a href="http://ceur-ws.org/Vol-3180/paper-253.pdf">Boromir</a></td><td>BERT, OCR, query-processing</td><td>0.878</td><td>0.768</td><td>0.425</td></tr>
<tr><td>Boromir</td><td>BERT, OCR, clustering, query-preprocessing</td><td>0.822</td><td>0.728</td><td>0.411</td></tr>
<tr><td>Boromir</td><td>AFINN, OCR</td><td>0.814</td><td>0.726</td><td>0.408</td></tr>
<tr><td>Minsc</td><td>Baseline</td><td>0.736</td><td>0.686</td><td>0.407</td></tr>
<tr><td>Boromir</td><td>AFINN, OCR, clustering</td><td>0.749</td><td>0.674</td><td>0.384</td></tr>
<tr><td>Boromir</td><td>AFINN, OCR, clustering, query-processing</td><td>0.767</td><td>0.688</td><td>0.382</td></tr>
<tr><td>Aramis</td><td>arg:formula, stance:formula</td><td>0.701</td><td>0.634</td><td>0.381</td></tr>
<tr><td>Aramis</td><td>arg:neural, stance: formula</td><td>0.687</td><td>0.632</td><td>0.365</td></tr>
<tr><td>Aramis</td><td>arg:neural, stance: neural</td><td>0.673</td><td>0.624</td><td>0.354</td></tr>
<tr><td>Jester</td><td>with emotion detection</td><td>0.696</td><td>0.647</td><td>0.350</td></tr>
<tr><td>Aramis</td><td>arg:formula, stance:neural</td><td>0.664</td><td>0.609</td><td>0.344</td></tr>
<tr><td>Jester</td><td>without emotion detection</td><td>0.671</td><td>0.618</td><td>0.336</td></tr>
<tr><td>Boromir</td><td>AFINN, clustering</td><td>0.600</td><td>0.545</td><td>0.319</td></tr>
</tbody>
</table>
<p>Scores are determined using <a href="https://github.com/dirkhovy/MACE">MACE</a> on crowdsourced relevance judgements by five independent annotators each on all submitted results (thus top-10). Expert annotations were used for uncertain cases (MACE confidence below 0.55).</p>


<h2><a id="related-work"></a>Related Work</h2>
<ul>
<li>Johannes Kiesel, Nico Reichenbach, Benno Stein, and Martin Potthast. <a href="https://webis.de/publications.html#kiesel_2021e">Image Retrieval for Arguments Using Stance-Aware Query Expansion.</a> 8th Workshop on Argument Mining (ArgMining 2021) at EMNLP, November 2021.</li>
<li>Dimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj Alam, Fabrizio Silvestri, Hamed Firooz, Preslav Nakov, and Giovanni Da San Martino. <a href="https://aclanthology.org/2021.semeval-1.7/">SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and Images</a>. 15th International Workshop on Semantic Evaluation (SemEval 2021), August 2021.</li>
<li>Keiji Yanai. <a href="https://dl.acm.org/doi/10.1145/1242572.1242816">Image collector III: a web image-gathering system with bag-of-keypoints.</a> 16th International Conference on World Wide Web (WWW 2007), May 2007.</li>
</ul>

<h2 id="task-committee">Task Committee</h2>
<div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
{% include people-cards/kiesel.html gender="male" %}
{% include people-cards/potthast.html gender="male" %}
{% include people-cards/stein.html gender="male" %}
</div>
<div class="uk-container uk-padding-large uk-padding-remove-bottom">
{% include organizations/clef-organizations-section.html year=2022 %}
</div>
</div>
</div>
</main>

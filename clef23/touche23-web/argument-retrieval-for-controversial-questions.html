---
layout: default
nav_active: shared-tasks
title: Touché at CLEF 2023 - Argument Retrieval for Controversial Questions
description: Touché at CLEF 2023 - Argument Retrieval for Controversial Questions
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="../../index.html">Touché</a></li>
<li><a href="../../shared-tasks.html">Shared Tasks</a></li>
<li class="uk-disabled"><a href="#">Argument Retrieval for Controversial Questions 2023</a></li>
</ul>
</nav>

<main class="uk-section uk-section-default">
<div class="uk-container">
<div class="uk-container uk-margin-small">
<h1 class="uk-margin-remove-top uk-margin-remove-bottom">Argument Retrieval for Controversial Questions 2023</h1>
<ul class="uk-list">
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task">Task</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#data">Data</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#evaluation">Evaluation</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#submission">Submission</a></li>
<!--<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#tira-quickstart">TIRA Quickstart</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#results">Results</a></li>-->
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
</ul>
</div>

<div class="uk-container uk-margin-medium">

<h2 id="synopsis">Synopsis</h2>
<ul>
<li>Task: Given a controversial topic, the task is to retrieve and rank documents by their relevance and argument quality and to detect the document stance.</li>
<li>Input: [<a href="../touche23-data/topics-task1.xml">topics</a>] <!--[<a href="https://doi.org/10.5281/zenodo.6801439">data</a>]--></li>
<li>Submission: will be added by mid-December.<!--[<a href="https://www.tira.io/task/touche-task-1/dataset/touche-task-1-2022-02-16">tira</a>]--></li>
<li>Evaluation: will be added by mid-December.<!--[<a href="https://zenodo.org/record/6801440/files/topics.xml?download=1">topics</a>]--></li>
<!--<li>Manual judgments (top-5 pooling): [qrels: <a href="https://zenodo.org/record/6801440/files/touche-task1-2022-relevance-dedup.qrels?download=1">relevance</a>, <a href="https://zenodo.org/record/6801440/files/touche-task1-2022-quality-dedup.qrels?download=1">quality</a>, <a href="https://zenodo.org/record/6801440/files/touche-task1-2022-coherence-dedup.qrels?download=1">coherence</a>]</li>-->
</ul>

<h2 id="task">Task</h2>
<p>The goal of <strong>Task 1</strong> is to to provide an overview of arguments and opinions on controversial topics. Given a controversial topic and a collection of web documents, the task is to retrieve and rank documents by relevance to the topic, by argument quality, and to detect the document stance. Participants of Task 2 will retrieve and rank documents that contain relevant causal evidence contained in the ClueWeb22 crawl for a given set of 50 search topics.</p>

<a class="uk-button uk-button-primary" href="https://clef2023-labs-registration.dei.unipd.it/">Register now</a>

<h2 id="data">Data</h2>
<p>Example topic for <strong>Task 1</strong> (<a href="../touche23-data/topics-task1.xml">download topics</a>):</p>
<pre><code>&lt;topic&gt;
&lt;number&gt;1&lt;/number&gt;
&lt;title&gt;Should teachers get tenure?&lt;/title&gt;
&lt;description&gt;A user has heard that some countries do give teachers tenure and others don't. 
Interested in the reasoning for or against tenure, the user searches for positive and negative arguments [...]&lt;/description&gt;
&lt;narrative&gt;Highly relevant arguments make a clear statement about tenure for teachers in schools or universities [...]&lt;/narrative&gt;
&lt;/topic&gt;</code></pre>
<p><strong>The corpus</strong> for Task 1 is ClueWeb22. If your organization already has a copy of the corpus, you can start with it; otherwise, we are currently preparing access to the data, which we expect to make available by mid-December.</p>
<!--<p><strong>The corpus</strong> for Task 1 is a pre-processed version of the <a href="https://zenodo.org/record/3734893">args.me corpus (version 2020-04-01)</a> where each argument is split into sentences; you may index these sentences (and the complete arguments if you wish to) with your favorite retrieval system. To ease participation, you may also directly use the <a href="https://www.args.me/index.html">args.me</a> search engine's <a href="https://www.args.me/api-en.html">API</a> for a baseline retrieval and then extract the candidate pair of sentences. Duplicate sentences are an expected part of this dataset. This is because in some of the debate portals from which the args.me corpus was derived, individual arguments are mapped to the same conclusion (which is essentially the discussion topic/title on the corresponding portal). Moreover, we opted to preserve the duplicates to reflect the common information retrieval scenario where filtering out such documents is part of the pipeline. [<a href="https://zenodo.org/record/6801440/files/args_processed_04_01.tar.gz?download=1">download</a>]</p>
-->

<p><strong>Additional resources:</strong></p>
<ul>
<li>Argument relevance and quality judgments: [<a href="https://webis.de/data.html?q=touche">Touché qrels</a>].</li>
<li>Argument mining tool: [<a href="https://demo.webis.de/targer/">TARGER</a>].</li>
</ul>


<h2 id="evaluation">Evaluation</h2>
<p>Our human assessors will label the ranked results both for their general topical relevance and for the rhetorical argument quality [<a href="https://webis.de/publications.html?q=wachsmuth_2017b">paper</a>], i.e., "well-writtennes": (1) whether the document contains arguments and whether the argument text has a good style of speech, (2) whether the text has a proper sentence structure and is easy to follow, (3) whether it includes profanity, has typos, etc. Optionally, detect the documents' stance: pro, con, neutral, or no stance towards the search topic. We will use nDCG@5 to evaluate rankings and accuracy to evaluate stance detection.</p>
<!--<p>The format of the relevance/quality judgment file:
<pre><code>qid 0 pair rel</code></pre>
With:
<ul>
<li><code>qid</code>: The topic number.</li>
<li><code>0</code>: Unused, always 0.</li>
<li><code>pair</code>: The pair of sentence IDs (from the provided version of the args.me corpus).</li>
<li><code>rel</code>: The relevance judgment: -2 non-argument (spam), 0 (not relevant) to 3 (highly relevant). The quality judgment: 0 (low) to 3 (high).</li>
</ul>
</p>
-->
<h2 id="submission">Submission</h2>
<p>We ask participants to use <a href="https://www.tira.io/">TIRA</a> for result submissions. <!--Please also have a look at our <a href="#tira-quickstart">TIRA quickstart</a>&#8212;in case of problems we will be able to assist you. Even though the preferred way of run submission is TIRA, in case of problems you may also submit runs via email. --></p>

<p>Runs may be either automatic or manual. An automatic run must not "manipulate" the topic titles via manual intervention. A manual run is anything that is not an automatic run. Upon submission, please let us know which of your runs are manual. For each topic, include up to 1,000 retrieved documents. Each team can submit <strong>up to 5</strong> different runs.</p>
<p>The submission format for the task will follow the standard TREC format:</p>
<pre><code>qid stance doc rank score tag</code></pre>
<p>With:</p>
<ul>
<li><code>qid</code>: The topic number.</li>
<li><code>stance</code>: The stance of the document (PRO: supports the topic, CON: against the topic, NEU: neutral stance, NO: no stance).</li>
<li><code>doc</code>: The document ID <code>qid</code>.</li>
<li><code>rank</code>: The rank the document is retrieved at.</li>
<li><code>score</code>: The score (integer or floating point) that generated the ranking. The score must be in descending (non-increasing) order. It is important to handle tied scores.</li>
<li><code>tag</code>: A tag that identifies your group and the method you used to produce the run.</li>
</ul>
<p>If you do not classify the stance, use <code>Q0</code> as the value in the stance column. The fields should be separated by a whitespace. The individual columns' widths are not restricted (i.e., score can be an arbitrary precision that has no ties) but it is important to include all columns and to separate them with a whitespace.<br><br>
An example run for Task 2 is:</p>
<pre><code>1 PRO clueweb22-en0010-85-29836___1 1 17.89 myGroupMyMethod
1 CON clueweb22-en0010-86-00457___3 2 16.43 myGroupMyMethod
1 NEU clueweb22-en0010-86-09202___5 3 16.32 myGroupMyMethod
...</code></pre>

<!--
<h2 id="tira-quickstart">TIRA Quickstart</h2>
<p>Participant software is run in a virtual machine. Log in to <a href="https://www.tira.io">TIRA</a>, go to the <a href="https://www.tira.io/task/touche-task-1/dataset/touche-task-1-2022-02-16">task's dataset page</a>, and click on "&gt;_ SUBMIT". Click the "CONNECTION INFO" button for how to connect to the virtual machine. Click on "POWER ON" if the state is not "RUNNING".</p>
<img width="75%" src="../touche22-figures/task1-tira-vm-overview.png" alt="Virtual machine state in TIRA.">
<p>The software is executed on the command line with two parameters: (1) <code>$inputDataset</code> refers to a directory that contains the <a href="#data">collection</a>; (2) <code>$outputDir</code> refers to a directory in which the software has to create the <a href="#submission">submission file</a> named <code>run.txt</code>. Specify exactly how each software of your virtual machine is run using the "Command" field in the TIRA web interface. Select <code>touche-task-1-2022-02-16</code> as the input dataset.</p>
<img width="75%" src="../touche22-figures/task1-tira-example-software.png" alt="Software configuration in TIRA.">
<p>As you "RUN" the software, you will not be able to connect to the virtual machine (takes at least 10 minutes). Once finished, click on "INSPECT" to check on the run and click on "EVALUATE" for a syntax check (give it a few minutes, then check back on the page). Your run will later be reviewed and evaluated by the organizers. If uncertain on something, ask in the <a href="https://www.tira.io/c/touche/9">forum</a> or send a mail/message to <a href="#task-committee">Shahbaz</a>.</p>
<img width="75%" src="../touche22-figures/task1-tira-example-evaluation.png" alt="A run in TIRA.">
<p>Create a separate "Software" entry in the TIRA web interface for each of your approaches. NOTE: By submitting your software you retain full copyrights. You agree to grant us usage rights for evaluation of the corresponding data generated by your software. We agree not to share your software with a third party or use it for any purpose other than research.</p>


<h2 id="results">Results</h2>
<table class="uk-table uk-table-divider uk-table-small uk-table-hover">
<caption>Best-scoring run of each team for relevance evaluation [<a href="argument-retrieval-for-controversial-questions-results-relevance.html">all runs</a>] [<a href="https://zenodo.org/record/6801440/files/touche-task1-2022-relevance-dedup.qrels?download=1">qrels</a>]</caption>
<thead>
<tr style="text-align: right;"> <th>Team</th> <th>Tag</th> <th>Mean NDCG@5</th> <th>CI95 Low</th> <th>CI95 High</th> </tr>
</thead>
<tbody>
<tr> <td>Porthos</td> <td>scl_dlm_bqnc_acl_nsp</td> <td>0.742</td> <td>0.670</td> <td>0.807</td> </tr>
<tr> <td>Daario Naharis</td> <td>INTSEG-Letter-no_stoplist-Krovetz-Icoef-Evidence-Par</td> <td>0.683</td> <td>0.609</td> <td>0.755</td> </tr>
<tr> <td>Bruce Banner</td> <td>Bruce-Banner_pyserinin_sparse_v3</td> <td>0.651</td> <td>0.573</td> <td>0.720</td> </tr>
<tr> <td>D Artagnan</td> <td>seupd2122-6musk-kstem-stop-shingle3</td> <td>0.642</td> <td>0.575</td> <td>0.705</td> </tr>
<tr> <td>Gamora</td> <td>seupd2122-javacafe-gamoraHeuristicsOnlyQueryReductionDoubleIndex</td> <td>0.616</td> <td>0.551</td> <td>0.687</td> </tr>
<tr> <td>Hit Girl</td> <td>Io</td> <td>0.588</td> <td>0.515</td> <td>0.657</td> </tr>
<tr> <td>Pearl</td> <td>PearlBlocklist_WeightedRelevance</td> <td>0.481</td> <td>0.399</td> <td>0.560</td> </tr>
<tr> <td>Gorgon</td> <td>GorgonA2Bm25</td> <td>0.408</td> <td>0.354</td> <td>0.461</td> </tr>
<tr> <td>General Grievous</td> <td>seupd2122-lgtm_QE_NRR</td> <td>0.403</td> <td>0.335</td> <td>0.471</td> </tr>
<tr> <td>Swordsman</td> <td>baseline_swordsman</td> <td>0.356</td> <td>0.296</td> <td>0.412</td> </tr>
<tr> <td>Korg</td> <td>korg9000</td> <td>0.252</td> <td>0.187</td> <td>0.318</td> </tr>
</tbody>
</table>
<table class="uk-table uk-table-divider uk-table-small uk-table-hover">
<caption>Best-scoring run of each team for quality evaluation [<a href="argument-retrieval-for-controversial-questions-results-quality.html">all runs</a>] [<a href="https://zenodo.org/record/6801440/files/touche-task1-2022-quality-dedup.qrels?download=1">qrels</a>]</caption>
<thead>
<tr style="text-align: right;"> <th>Team</th> <th>Tag</th> <th>Mean NDCG@5</th> <th>CI95 Low</th> <th>CI95 High</th> </tr>
</thead>
<tbody>
<tr> <td>Daario Naharis</td> <td>INTSEG-Letter-no_stoplist-Krovetz-Icoef-Evidence-Par</td> <td>0.913</td> <td>0.870</td> <td>0.947</td> </tr>
<tr> <td>Porthos</td> <td>scl_dlm_bqnc_acl_nsp</td> <td>0.873</td> <td>0.825</td> <td>0.913</td> </tr>
<tr> <td>Gamora</td> <td>seupd2122-javacafe-gamoraHeuristicsOnlyQueryReductionDoubleIndex</td> <td>0.785</td> <td>0.729</td> <td>0.848</td> </tr>
<tr> <td>Hit Girl</td> <td>Ganymede</td> <td>0.776</td> <td>0.707</td> <td>0.840</td> </tr>
<tr> <td>Bruce Banner</td> <td>Bruce-Banner_pyserinin_sparse_v1</td> <td>0.772</td> <td>0.702</td> <td>0.830</td> </tr>
<tr> <td>Gorgon</td> <td>GorgonA2Bm25</td> <td>0.742</td> <td>0.700</td> <td>0.786</td> </tr>
<tr> <td>D Artagnan</td> <td>seupd2122-6musk-stop-wordnet-kstem-dirichlet</td> <td>0.733</td> <td>0.676</td> <td>0.787</td> </tr>
<tr> <td>Pearl</td> <td>PearlArgRank7530</td> <td>0.678</td> <td>0.609</td> <td>0.744</td> </tr>
<tr> <td>Swordsman</td> <td>baseline_swordsman</td> <td>0.608</td> <td>0.543</td> <td>0.671</td> </tr>
<tr> <td>General Grievous</td> <td>seupd2122-lgtm_NQE_NRR</td> <td>0.517</td> <td>0.442</td> <td>0.591</td> </tr>
<tr> <td>Korg</td> <td>korg9000</td> <td>0.453</td> <td>0.384</td> <td>0.529</td> </tr>
</tbody>
</table>
<table class="uk-table uk-table-divider uk-table-small uk-table-hover">
<caption>Best-scoring run of each team for coherence evaluation [<a href="argument-retrieval-for-controversial-questions-results-coherence.html">all runs</a>] [<a href="https://zenodo.org/record/6801440/files/touche-task1-2022-coherence-dedup.qrels?download=1">qrels</a>]</caption>
<thead>
<tr style="text-align: right;"> <th>Team</th> <th>Tag</th> <th>Mean NDCG@5</th> <th>CI95 Low</th> <th>CI95 High</th> </tr>
</thead>
<tbody>
<tr> <td>Daario Naharis</td> <td>INTSEG-Run-Whitespace-Krovetz-Stoplist-Pos-Evidence-icoeff-Sep</td> <td>0.458</td> <td>0.389</td> <td>0.525</td> </tr>
<tr> <td>Porthos</td> <td>scl_dlm_bqnc_acl_nsp</td> <td>0.429</td> <td>0.353</td> <td>0.509</td> </tr>
<tr> <td>Pearl</td> <td>PearlArgRank7530</td> <td>0.398</td> <td>0.311</td> <td>0.485</td> </tr>
<tr> <td>Bruce Banner</td> <td>Bruce-Banner_pyserinin_sparse_v1</td> <td>0.378</td> <td>0.300</td> <td>0.459</td> </tr>
<tr> <td>D Artagnan</td> <td>seupd2122-6musk-kstem-stop-shingle3</td> <td>0.378</td> <td>0.311</td> <td>0.452</td> </tr>
<tr> <td>Hit Girl</td> <td>Ganymede</td> <td>0.377</td> <td>0.303</td> <td>0.456</td> </tr>
<tr> <td>Gamora</td> <td>seupd2122-javacafe-gamora_sbert_kstemstopengpos_multi_YYY</td> <td>0.285</td> <td>0.203</td> <td>0.373</td> </tr>
<tr> <td>Gorgon</td> <td>GorgonKEBM25</td> <td>0.282</td> <td>0.233</td> <td>0.335</td> </tr>
<tr> <td>Swordsman</td> <td>baseline_swordsman</td> <td>0.248</td> <td>0.193</td> <td>0.303</td> </tr>
<tr> <td>General Grievous</td> <td>seupd2122-lgtm_QE_NRR</td> <td>0.231</td> <td>0.162</td> <td>0.313</td> </tr>
<tr> <td>Korg</td> <td>korg9000</td> <td>0.168</td> <td>0.117</td> <td>0.223</td> </tr>
</tbody>
</table>
-->
<h2 id="task-committee">Task Committee</h2>
<div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
{% include people-cards/bondarenko.html %}
{% include people-cards/froebe.html %}
{% include people-cards/schlatt.html %}
{% include people-cards/reimer.html %}
{% include people-cards/stein.html %}
{% include people-cards/potthast.html %}
{% include people-cards/hagen.html %}
</div>

<div class="uk-container uk-padding-large uk-padding-remove-bottom">
{% include organizations/clef-organizations-section.html year=2023 %}
</div>
</div>
</div>
</main>

---
layout: default
nav_active: shared-tasks
title: Touché at CLEF 2023 - Intra-Multilingual Multi-Target Stance Classification
description: Touché at CLEF 2023 - Intra-Multilingual Multi-Target Stance Classification
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="../../index.html">Touché</a></li>
<li><a href="../../shared-tasks.html">Shared Tasks</a></li>
<li class="uk-disabled"><a href="#">Intra-Multilingual Multi-Target Stance Classification 2023</a></li>
</ul>
</nav>

<main class="uk-section uk-section-default">
<div class="uk-container">
<div class="uk-container uk-margin-small">
<h1 class="uk-margin-remove-top uk-margin-remove-bottom">Intra-Multilingual Multi-Target Stance Classification 2023</h1>
<ul class="uk-list">
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task">Task</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#data">Data</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#evaluation">Evaluation</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#submission">Submission</a></li>
<!--<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#tira-quickstart">TIRA Quickstart</a></li>
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#results">Results</a></li> -->
<li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
</ul>
</div>

<div class="uk-container uk-margin-medium">

<h2 id="synopsis">Synopsis</h2>
<ul>
<li>Task: Given a proposal on a socially important issue, the task is to classify whether a comment is in favor, against, or neutral towards the proposal.</li>
<li>Input: <a href="#data">data</a><!--[<a href="https://doi.org/10.5281/zenodo.6801439">data</a>]--></li>
<li>Submission: [<a href="https://www.tira.io/task/touche-2023-task-4">submit</a>].<!--[<a href="https://www.tira.io/task/touche-task-1/dataset/touche-task-1-2022-02-16">tira</a>]--></li>
<!--<li>Evaluation: TBA[<a href="https://zenodo.org/record/6801440/files/topics.xml?download=1">topics</a>]</li>-->
<!--<li>Manual judgments (top-5 pooling): [qrels: <a href="https://zenodo.org/record/6802592/files/touche-task2-2022-relevance.qrels?download=1">relevance</a>, <a href="https://zenodo.org/record/6802592/files/touche-task2-2022-quality.qrels?download=1">quality</a>, <a href="https://zenodo.org/record/6802592/files/touche-task2-2022-stance.qrels?download=1">stance</a>]</li>-->
</ul>

<h2 id="task">Task</h2>
<p>The goal of <strong>Task 4</strong> is to support to support opinion formation on socially important topics. Given a proposal on a socially important issue, its title and topic, the task is to classify whether a comment is <em>in favor</em>, <em>against</em>, or <em>neutral</em> towards the proposal. The proposals and titles can be written in any of the 24 EU languages (plus Catalan and Esperanto) and will come with their automatic English translation.</p>

<p><strong>Subtask 1: Cross-Debate Classification.</strong> In the first subtask, the participants cannot use in their training set the examples from debates that are in the test set.</p> 
<p><strong>Subtask 2: All-data-available.</strong> In the second subtask, the participants can use all the available data, labeled or unlabeled.</p>

<a class="uk-button uk-button-primary" href="https://clef2023-labs-registration.dei.unipd.it/">Register now</a>

<h2 id="data">Data</h2>

<p>The data used in this Shared Task comes from the CoFE dataset, presented at AACL-IJCNLP 2022 [<a href="https://drive.google.com/file/d/1ARE-Po6rWEvOkTuaCVOZpSj0Eb9qpTk8/view?usp=sharing">oral presentation</a>] [<a href="https://drive.google.com/file/d/1D3ao_A3bKx-Xd1JK0c8t8Uv_x6NSGU8N/view?usp=sharing">slides</a>].</p>

<p><strong>CF_S</strong> dataset is composed of 7K comments from CoFE that are annotated with stance by the writer. The annotation is binary: <em>in favor</em> or <em>against</em>. It can be found in the column "alignment" .</p>

<p><strong>CF_U</strong> dataset is composed of 12K unlabeled comments from CoFE. All the "alignment" are "None".</p>

<p><strong>CF_E-Dev</strong> dataset is composed of multiligual comments that have been annotated by external coders (not the writer of the comment) in a 3-class fashion. It is consider as silver-standard because of a low inter-annotator agreement. The labels are in the columns "label". Note that all the "alignment" of those comments are "None". </p> 

<p><strong>CF_E-Test</strong> dataset is composed of 1.2K comments in English, French, German, Italian, Greek, and Hungarian that have been annotated by external coders (not the writer of the comment) in a 3-class fashion. It is considered as gold-standard because of a higher inter-annotator agreement. This dataset will used for the test phase. This test set will be available a few days before the test phase.</p>

<p>The datasets to use for the <strong>Subtask 1: Cross-Debate Classification</strong>:</p>

<ul>
<li>CF_U without the comments from the debates of the test set CF_E-Test is available [<a href="https://drive.google.com/file/d/1Sf3j0phfcCwD3ZYmNuUdOYlc-5yV8Da2/view?usp=sharing">here</a>].</li>
<li>CF_S without the comments from the debates of test set CF_E-Test is available [<a href="https://drive.google.com/file/d/1SbVLDIM8aARTUdwxJESM-6bucsLz-wk0/view?usp=sharing">here</a>].</li>
<li>CF_E-Dev without the comments from debates of the test set CF_E-Test is available [<a href="https://drive.google.com/file/d/1SfaGsZu7P3JKyLXckpitXpWqqSsRmVeN/view?usp=sharing">here</a>].</li>
</ul>

<p>The datasets to use for the <strong>Subtask 2: All-data-available</strong>:</p>

<ul>
<li>CF_U is available [<a href="https://drive.google.com/file/d/1SOlTUmlIhdIKKeGVVTwyNihApgQAC_Nd/view?usp=sharing">here</a>].</li>
<li>CF_S is available [<a href="https://drive.google.com/file/d/1SOiEDLdlIzWpWJlSBMQoP9TfUqBBMHTv/view?usp=sharing">here</a>].</li>
<li>CF_E-Dev is available [<a href="https://drive.google.com/file/d/1SKQhew5AdQw59oGsawHzqy4GZ9GU-OcN/view?usp=sharing">here</a>].</li>
</ul>

<p>The teams are free to use every other datasets they want to.</p>

<p>All the <strong>proposals</strong> are in a separate file containing all the 4.2K proposals. The column "id" of the proposal file corresponds to the column "id_prop" of the comment files. [<a href="https://drive.google.com/file/d/1R-eAEghjAns6CjC7P48DH1wOWYKHcvEx/view?usp=sharing">download</a>].</p>

<p>Additional metadata:</p>

<ul>
<li>For the proposals: the title of the proposal, the topic, the ID of the writter, the language of the text, and the number of endorsments.</li>
<li>For the comments: the ID of the writter, the language of the text and the number of up/downvote.</li>
</ul>

<p>Example <strong>proposal</strong> instance for <strong>Task 4:</strong></p>

<table class="uk-table uk-table-divider uk-table-small uk-table-hover">
<thead>
<tr style="text-align: right;"> <th>id</th> <th>title</th> <th>proposal</th> <th>proposal_en</th> <th>title_en</th> <th>topic</th> <th>lan</th> <th>endors.</th> </tr>
</thead>
<tbody>
<tr>
<td>43992</td>
<td>Massentierhalter EU-Agrarsubventionen</td>
<td>Die EU hat "schöne" Ideen und arbeitet an Gesetzte für den Tierschutz und die Umwelt. Doch solange immer noch Massentierbetriebe von der EU durch Subventionen unterstützt werden, ist dies alles nicht glaubwürdig!</td>
<td>The EU has "beautiful" ideas and is working on legislation for animal welfare and the environment. But as long as EU subsidies still support mass animal farms, all of this is not credible!</td>
<td>Mass livestock farmers EU agricultural subsidies</td>
<td>GreenDeal</td>
<td>de</td>
<td>1</td>
</tr>
<tr>
<td>11138</td>
<td>Författningsdomstol</td>
<td>För att säkra skyddet av konstitutionen i varje EU-land bör krav ställas av EU att en oberoende konstitutionsdomstol finns i varje land. Exempelvis saknas en sådan i Sverige vilket leder till låg kvalité på stiftade lagar samt lagar som inte är anpassade till grundlagen.</td>
<td>In order to ensure the protection of the Constitution in each EU country, the EU should be required to have an independent constitutional court in each country. For example, there is no one in Sweden, which results in a low quality of enacted laws and laws that are not in line with the Constitution.</td>
<td>Constitutional court</td>
<td>ValuesRights</td>
<td>sv</td>
<td>0</td>
</tr>
</tbody>
</table>

<p>Example <strong>comment</strong> instance for <strong>Task 4:</strong></p>

<table class="uk-table uk-table-divider uk-table-small uk-table-hover">
<thead>
<tr style="text-align: right;"> <th>id</th> <th>id_prop</th> <th>alignment</th> <th>comment</th> <th>depth</th> <th>thread_id</th> <th>last_ comment_ in_thread</th> <th>upvote</th> <th>downvote</th> <th>Topic</th> <th>lan</th> <th>time</th> </tr>
</thead>
<tbody>
<tr>
<td>comment_ 90817</td>
<td>89992</td>
<td>Against</td>
<td>Europäische Waffenexporte leisten einen großen Beitrag zum zum Elend in der Welt [...]</td>
<td>0</td>
<td>comment_ 90817</td>
<td>True</td>
<td>0</td>
<td>0</td>
<td>EUInTheWorld</td>
<td>de</td>
<td>2021-09-14 T10:28:46+02:00</td>
</tr>
<tr>
<td>comment_ 1330</td>
<td>417</td>
<td>Against</td>
<td>Je ne comprend pas très bien ce que l'on reproche a l'UE (vous devriez pouvoir m'éclairer) [...]</td>
<td>0</td>
<td>comment_ 1330</td>
<td>True</td>  
<td></td>
<td>0</td>
<td>ValuesRights</td>
<td>fr</td>
<td>2021-04-22 T18:55:25+02:00</td>
</tr>
</tbody>
</table>
<!--<p><strong>The corpus</strong> for Task 2 is a collection of about 0.9 million text passages. [<a href="https://zenodo.org/record/6802592/files/touche-task2-passages-version-002.jsonl.gz?download=1">download</a>]</p>
<p><strong>Other data</strong> for this task are the topics, quality judgements, and for training a subset of MS MARCO with comparative questions and a collection of text passages expanded with queries generated using DocT5Query. [<a href="https://doi.org/10.5281/zenodo.6797875">download</a>] [<a href="https://github.com/touche-webis-de/touche-code/blob/main/clef22/argument-retrieval-for-comparative-questions/parse_topics.ipynb">topic parser</a>]</p>-->

<h2 id="evaluation">Evaluation</h2>
<p>Results will be evaluated using a micro-averaged F1 over the 3 stance classes.</p>
<!--
<p>The format of the relevance/quality judgment file:
<pre><code>qid 0 doc rel</code></pre>
With:
<ul>
<li><code>qid</code>: The topic number.</li>
<li><code>0</code>: Unused, always 0.</li>
<li><code>doc</code>: The document ID ("trec_id" if you use ChatNoir or the official ClueWeb12 ID).</li>
<li><code>rel</code>: The relevance judgment: 0 (not relevant) to 3 (highly relevant). The quality judgment: 0 (low, or no arguments) to 3 (high).</li>
</ul>
You can use the corresponding <a href="https://github.com/touche-webis-de/touche-code/blob/main/clef22/evaluate.py">evaluation script</a> to evaluate your run using the relevance judgments.</p>
-->
<h2 id="submission">Submission</h2>
<p>We ask participants to use <a href="https://www.tira.io/">TIRA</a> for result submissions. <!--Please also have a look at our <a href="#tira-quickstart">TIRA quickstart</a>&#8212;in case of problems we will be able to assist you. Even though the preferred way of run submission is TIRA, in case of problems you may also submit runs via email. --></p>

  
<h3>TIRA Tutorial and TIRA Baselines</h3>
<p>
We provide a TIRA tutorial that provides baselines that can be executed in TIRA at <a href="https://github.com/touche-webis-de/touche-code/tree/main/clef23">https://github.com/touche-webis-de/touche-code/tree/main/clef23</a>.
</p>


<!--
<h2 id="tira-quickstart">TIRA Quickstart</h2>
<p>Participants have to upload (through SSH or RDP) their retrieval models in a dedicated TIRA virtual machine, so that their runs can be reproduced and so that they can be easily applied to different data (of same format) in the future. You can find host ports for your VM in the web interface, same login as to your VM.  If you cannot connect to your VM, please make sure it is powered on: you can check and power on your machine in the web interface where you see the state of your VM and can access the connection informations on how to access your VM.</p>
<img width="75%" style="margin-left: auto; margin-right: auto; display: block;" src="../touche22-figures/task2-tira-vm-overview.png" alt="Overview of the virtual machine in TIRA.">
<p>Your software is expected to accept two arguments:
<ul>
<li>An input directory (named <code>$inputDataset</code> in TIRA). This input directory contains a <code>topics.xml</code> file that contains the topics for which documents should be retrieved and a <code>passages.jsonl.gz</code> that contains the passages.</li>
<li>An output directory (named <code>$outputDir</code> in TIRA). Your software should create a standard trec run file in <code>$outputDir/run.txt</code>.</li>
</ul>
As soon as your Software is installed in your VM, you can register it in TIRA. Assume that your software is started with a bash script in your home directory called <code>my-software.sh</code> which expects an argument <code>-i</code> specifying the input directory, and an argument <code>-o</code> specifying the output directory. Click on "Add software" and specify the command <code>./my-software.sh -i $inputDataset -o $outputDir</code>. Please select `touche-2022-task2` as input dataset.</p>
<img width="75%" style="margin-left: auto; margin-right: auto; display: block;" src="../touche22-figures/task2-tira-example-software.png" alt="Overview of the software configuration in TIRA.">
<p>Please save your software and click on "Run" to execute your software in TIRA. Note that your VM will not be accessible while your system is running – it will be “sandboxed”, detached from the internet, and after the run the state of the VM before the run will be restored. Your run will be reviewed and evaluated by the organizers.</p>
<p>NOTE: By submitting your software you retain full copyrights. You agree to grant us usage rights for evaluation of the corresponding data generated by your software. We agree not to share your software with a third party or use it for any purpose other than research.</p>
<p>Once the run of your system completes, please also run the evaluator on the output of your system to verify that your output is a valid submission. These are two separate actions and both should be invoked through the web interface of TIRA. You don’t have to install the evaluator in your VM. It is already prepared in TIRA. You can run the evaluator in the overview of your runs by clicking on the "Evaluate" button.</p>
<img width="75%" style="margin-left: auto; margin-right: auto; display: block;" src="../touche22-figures/task2-tira-example-evaluation.png" alt="Overview of the software evaluation in TIRA.">
<p>By Clicking on "Inspect" you can see and download the STDOUT and STDERR as well as the outputs of your system. The output of the evaluator will tell you if the output of your run valid. If you think something went wrong with your run, send us an e-mail or open a <a href="https://www.tira.io/c/touche/9">new Topic in the Touché Forum in TIRA</a>. Additionally, we review your submissions and contact you on demand.</p>
<p>You can register more than one system (“software/ model”) per virtual machine using the web interface. TIRA gives systems automatic names “Software 1”, “Software 2” etc. You can perform several runs per system.</p>

<h2 id="results">Results</h2>
<table class="uk-table uk-table-divider uk-table-small uk-table-hover">
<caption>Best-scoring run of each team for relevance evaluation [<a href="argument-retrieval-for-comparative-questions-results-relevance.html">all runs</a>] [<a href="https://zenodo.org/record/6802592/files/task2_relevance_results_full.csv?download=1">per-topic</a>] [<a href="https://zenodo.org/record/6802592/files/touche-task2-2022-relevance.qrels?download=1">top-5 pooled qrels</a>]</caption>
<thead>
<tr style="text-align: right;"> <th>Team</th> <th>Tag</th> <th>Mean nDCG@5</th> <th>CI95 Low</th> <th>CI95 High</th> </tr>
</thead>
<tbody>
<tr> <td>Captain Levi</td> <td>levirank_dense_initial_retrieval</td> <td>0.758</td> <td>0.708</td> <td>0.810</td> </tr>
<tr> <td>Aldo Nadi</td> <td>seupd2122-kueri_rrf_reranked</td> <td>0.709</td> <td>0.649</td> <td>0.769</td> </tr>
<tr> <td>Katana</td> <td>Colbert edinburg</td> <td>0.618</td> <td>0.551</td> <td>0.676</td> </tr>
<tr> <td>Captain Tempesta</td> <td>hextech_run_1</td> <td>0.574</td> <td>0.504</td> <td>0.640</td> </tr>
<tr> <td>Olivier Armstrong</td> <td>tfid_arg_similarity</td> <td>0.492</td> <td>0.414</td> <td>0.569</td> </tr>
<tr> <td>Puss in Boots</td> <td>BM25-Baseline</td> <td>0.469</td> <td>0.403</td> <td>0.538</td> </tr>
<tr> <td>Grimjack</td> <td>grimjack-fair-reranking-argumentative-axioms</td> <td>0.422</td> <td>0.351</td> <td>0.496</td> </tr>
<tr> <td>Asuna</td> <td>asuna-run-5</td> <td>0.263</td> <td>0.202</td> <td>0.324</td> </tr>
</tbody>
</table>
<table class="uk-table uk-table-divider uk-table-small uk-table-hover">
<caption>Best-scoring run of each team for quality evaluation [<a href="argument-retrieval-for-comparative-questions-results-quality.html">all runs</a>] [<a href="https://zenodo.org/record/6802592/files/task2_quality_results_full.csv?download=1">per-topic</a>] [<a href="https://zenodo.org/record/6802592/files/touche-task2-2022-quality.qrels?download=1">top-5 pooled qrels</a>]</caption>
<thead>
<tr style="text-align: right;"> <th>Team</th> <th>Tag</th> <th>Mean nDCG@5</th> <th>CI95 Low</th> <th>CI95 High</th> </tr>
</thead>
<tbody>
<tr> <td>Aldo Nadi</td> <td>seupd2122-kueri_RF_reranked</td> <td>0.774</td> <td>0.719</td> <td>0.828</td> </tr>
<tr> <td>Captain Levi</td> <td>levirank_dense_initial_retrieval</td> <td>0.744</td> <td>0.690</td> <td>0.804</td> </tr>
<tr> <td>Katana</td> <td>Colbert trained by me</td> <td>0.644</td> <td>0.575</td> <td>0.707</td> </tr>
<tr> <td>Captain Tempesta</td> <td>hextech_run_5</td> <td>0.597</td> <td>0.523</td> <td>0.674</td> </tr>
<tr> <td>Olivier Armstrong</td> <td>tfid_arg_similarity</td> <td>0.582</td> <td>0.506</td> <td>0.662</td> </tr>
<tr> <td>Puss in Boots</td> <td>BM25-Baseline</td> <td>0.476</td> <td>0.401</td> <td>0.556</td> </tr>
<tr> <td>Grimjack</td> <td>grimjack-fair-reranking-argumentative-axioms</td> <td>0.403</td> <td>0.330</td> <td>0.478</td> </tr>
<tr> <td>Asuna</td> <td>asuna-run-5</td> <td>0.332</td> <td>0.254</td> <td>0.418</td> </tr>
</tbody>
</table>
<table class="uk-table uk-table-divider uk-table-small uk-table-hover">
<caption>Best-scoring run of each team for stance evaluation with <code>N_run</code>, <code>N_team</code> being the number of stance predictions in the run or team that were manually labeled [<a href="argument-retrieval-for-comparative-questions-results-stance.html">all runs</a>] [<a href="https://zenodo.org/record/6802592/files/task2_stance_results_full.csv?download=1">per-topic</a>] [<a href="https://zenodo.org/record/6802592/files/touche-task2-2022-stance.qrels?download=1">top-5 pooled qrels</a>]</caption>
<thead>
<tr style="text-align: right;"> <th>Team</th> <th>Tag</th> <th>F1_macro_run</th> <th>N_run</th> <th>F1_macro_team</th> <th>N_team</th> </tr>
</thead>
<tbody>
<tr> <td>Grimjack</td> <td>grimjack-all-you-need-is-t0</td> <td>0.313</td> <td>1208</td> <td>0.235</td> <td>1386</td> </tr>
<tr> <td>Captain Levi</td> <td>levirank_dense_initial_retrieval</td> <td>0.301</td> <td>1688</td> <td>0.261</td> <td>2020</td> </tr>
<tr> <td>Katana</td> <td>Colbert edinburg</td> <td>0.229</td> <td>1027</td> <td>0.220</td> <td>1301</td> </tr>
<tr> <td>Olivier Armstrong</td> <td>tfid_arg_similarity</td> <td>0.191</td> <td>551</td> <td>0.191</td> <td>551</td> </tr>
<tr> <td>Puss in Boots</td> <td>Always-NO-Baseline</td> <td>0.158</td> <td>1328</td> <td>0.158</td> <td>1328</td> </tr>
<tr> <td>Asuna</td> <td>asuna-run-5</td> <td>0.106</td> <td>578</td> <td>0.106</td> <td>578</td> </tr>
</tbody>
</table>
-->
<h2 id="task-committee">Task Committee</h2>
<div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
{% include people-cards/barriere.html %}
{% include people-cards/hemamou.html %}
{% include people-cards/luck.html %}
{% include people-cards/ravenet.html %}
</div>
<div class="uk-container uk-padding-large uk-padding-remove-bottom">
{% include organizations/clef-organizations-section.html year=2023 %}
</div>
</div>
</div>
</main>
